<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/notes/rss.xml" title="notes Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/notes/atom.xml" title="notes Blog Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"><title data-react-helmet="true">Meta Label Correction for Noisy Label Learning (MLC) | notes</title><meta data-react-helmet="true" property="og:title" content="Meta Label Correction for Noisy Label Learning (MLC) | notes"><meta data-react-helmet="true" name="description" content="MLC"><meta data-react-helmet="true" property="og:description" content="MLC"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><link data-react-helmet="true" rel="shortcut icon" href="/notes/img/favicon.ico"><link rel="stylesheet" href="/notes/styles.58026286.css">
<link rel="preload" href="/notes/styles.36e43167.js" as="script">
<link rel="preload" href="/notes/runtime~main.9fb735cd.js" as="script">
<link rel="preload" href="/notes/main.6c708b17.js" as="script">
<link rel="preload" href="/notes/1.e087ac29.js" as="script">
<link rel="preload" href="/notes/2.af5c63dd.js" as="script">
<link rel="preload" href="/notes/ccc49370.852fedf5.js" as="script">
<link rel="preload" href="/notes/a344bd16.f91d91f1.js" as="script">
<link rel="preload" href="/notes/0c87fc9f.44bf6fde.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/notes/"><strong class="navbar__title">notes</strong></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/pierrenodet/notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/notes/"><strong class="navbar__title">notes</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a href="https://github.com/pierrenodet/notes" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper blog-wrapper"><div class="container margin-vert--lg"><div class="row"><div class="col col--2"><div class="sidebar_SWld thin-scrollbar"><h3 class="sidebarItemTitle_Km2m">Recent posts</h3><ul class="sidebarItemList_3UpA"><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/03/01/mentornet">MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels (MentorNet)</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/02/26/coteaching">Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching)</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/02/24/mwnet">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)</a></li><li class="sidebarItem_2T0D"><a aria-current="page" class="sidebarItemLink_v5H9 sidebarItemLinkActive_1anX" href="/notes/2021/02/19/mlc">Meta Label Correction for Noisy Label Learning (MLC)</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2020/02/29/mutual-information">Flexible Biquality Learning with Mutual Information</a></li></ul></div></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_3-lP">Meta Label Correction for Noisy Label Learning (MLC)</h1><div class="margin-vert--md"><time datetime="2021-02-19T00:00:00.000Z" class="blogPostDate_Ta7i">February 19, 2021  · 4 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"></div></div></header><section class="markdown"><p align="center"></p><p><img alt="MLC" src="/notes/assets/images/mlc-c349b4ef10e45681ad6f2384831d06ee.png"></p><p></p><ul><li><strong>code</strong> : <a href="https://github.com/microsoft/MLC" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/MLC</a></li><li><strong>pdf</strong> : <a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/12/aaai2021_mlc_zheng.pdf" target="_blank" rel="noopener noreferrer">https://www.microsoft.com/en-us/research/uploads/prod/2020/12/aaai2021_mlc_zheng.pdf</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><ol><li>MLC is an actual biquality learning algorithm (uses both trusted and untrusted data).</li><li>MLC uses a Meta Learning approach to correct corrupted/untrusted labels : Learning to correct.</li><li>The meta model (a label correction network) correct label for corrupted instances while the main model learns on corrected labels.</li><li>Both models are jointly trained by solving bilevel optimization. The meta model is learned by how much the main model performs on trusted labels. The main model is learned by how much it performs on labels corrected by the meta model.</li><li>A (novel) k step look ahead SGD to solve this optimization problem.</li><li>Meta Model has an embedding of the data and the untrusted label as an input to learn how to correct it.</li><li>Experiments on widely used datasets and noises against State-of-the-Art competitors.</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="assets"></a>Assets<a class="hash-link" href="#assets" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="strengths"></a>Strengths<a class="hash-link" href="#strengths" title="Direct link to heading">#</a></h3><ol><li>MLC uses untrusted samples by correcting their labels, works well on assymetrical noise</li><li>Meta Model has an embedding of the data and the untrusted label as an input to learn how to correct it.</li><li>MLC and k-step look ahead SGD works well empirically.</li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="drawbacks"></a>Drawbacks<a class="hash-link" href="#drawbacks" title="Direct link to heading">#</a></h3><ol><li>Only agggregated results in experiments, hard to know when it&#x27;s better than the competitors.</li><li>Costly algorithm especially with at least 5 steps ahead SGD, experiments are made with pretrained embeddigns (ResNet/BERT).</li><li>Lack of theoritical guarantees (bilevel optimization heuristic only works empirically).</li><li>Less interpretable than competitors, for example GLC has an explicit transition matrix to correct labels, MLC use a MLP (blackbox) over embeddings of a resnet/bert.</li><li>Correct Labels in an hard manner (don&#x27;t use classifier confidence to do both label correction and instance reweighting).</li><li>Meta Learning algorithm (can be hard to implement for a wide spread of neural network architectures and optimizers). Thanksfully some library solve this problem (see higher for PyTorch).</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="novelty"></a>Novelty<a class="hash-link" href="#novelty" title="Direct link to heading">#</a></h2><p>It’s a Novel/Somewhat Novel approach that reapplies an idea found in another paper (mwnet) to learn how to relabel a noisy sample by using bilevel joint optimization. K step look ahead SGD might be novel too.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="significance"></a>Significance<a class="hash-link" href="#significance" title="Direct link to heading">#</a></h2><p>The results are moderately significant as the improvements in accuracy are not huge over GLC which is a way simpler algorithm to both implement, test and interpret (not a blackbox MLP to correct labels). Moreover the results in experiments are quite aggregated so it’s not obvious where this method adds benefits (kind of label noise, strength of label noise, datasets).</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="soudness"></a>Soudness<a class="hash-link" href="#soudness" title="Direct link to heading">#</a></h2><p>The soundness of the article seems ok as it reuses a lot of results found in other papers to justify the algorithm.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="evaluation"></a>Evaluation<a class="hash-link" href="#evaluation" title="Direct link to heading">#</a></h2><p>It&#x27;s sufficient as it reuses a lot of ideas found in other papers and the reported performances seems ok.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-can-i-useenhance-the-paper-"></a>How can I use/enhance the paper ?<a class="hash-link" href="#how-can-i-useenhance-the-paper-" title="Direct link to heading">#</a></h2><ol><li>Combine both instance reweighting and label correction, by either combining MWNet and MLC or use the soft labels instead of hard labels provided by the meta model (the weight comes from the meta model confidence).</li><li>Doest it work well for not at random noise (it should but no experimentns have been conducted)</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="what-did-i-learn-from-reading-this-paper-"></a>What did I learn from reading this paper ?<a class="hash-link" href="#what-did-i-learn-from-reading-this-paper-" title="Direct link to heading">#</a></h2><p>Better understanding of how meta learning works.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="new-paper-to-readinteresting-citations"></a>New paper to read/Interesting Citations<a class="hash-link" href="#new-paper-to-readinteresting-citations" title="Direct link to heading">#</a></h2><ol><li>MWNet (predecessor)</li><li>DART (k-step look ahead SGD)</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-i-am-going-to-use-this-paper-"></a>How I am going to use this paper ?<a class="hash-link" href="#how-i-am-going-to-use-this-paper-" title="Direct link to heading">#</a></h2><p>To implement.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="bibtex"></a>Bibtex<a class="hash-link" href="#bibtex" title="Direct link to heading">#</a></h2><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">@article{zheng2021mlc,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  title={Meta Label Correction for Noisy Label Learning},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  author={Zheng, Guoqing and Awadallah, Ahmed Hassan and Dumais, Susan},  </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  journal={Proceedings of the AAAI Conference on Artificial Intelligence},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  volume={35},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  year={2021},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">}</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/notes/tags/reading-notes">reading-notes</a><a class="margin-horiz--sm" href="/notes/tags/meta-learning">meta-learning</a><a class="margin-horiz--sm" href="/notes/tags/label-correction">label-correction</a></div></footer></article><div><a href="https://github.com/pierrenodet/notes/edit/master/notes/2021-02-19-mlc.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2LL7"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="margin-vert--xl"><nav class="pagination-nav" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/notes/2021/02/24/mwnet"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/notes/2020/02/29/mutual-information"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Flexible Biquality Learning with Mutual Information »</div></a></div></nav></div></main><div class="col col--2"><div class="tableOfContents_2xL- thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link">Summary</a></li><li><a href="#assets" class="table-of-contents__link">Assets</a><ul><li><a href="#strengths" class="table-of-contents__link">Strengths</a></li><li><a href="#drawbacks" class="table-of-contents__link">Drawbacks</a></li></ul></li><li><a href="#novelty" class="table-of-contents__link">Novelty</a></li><li><a href="#significance" class="table-of-contents__link">Significance</a></li><li><a href="#soudness" class="table-of-contents__link">Soudness</a></li><li><a href="#evaluation" class="table-of-contents__link">Evaluation</a></li><li><a href="#how-can-i-useenhance-the-paper-" class="table-of-contents__link">How can I use/enhance the paper ?</a></li><li><a href="#what-did-i-learn-from-reading-this-paper-" class="table-of-contents__link">What did I learn from reading this paper ?</a></li><li><a href="#new-paper-to-readinteresting-citations" class="table-of-contents__link">New paper to read/Interesting Citations</a></li><li><a href="#how-i-am-going-to-use-this-paper-" class="table-of-contents__link">How I am going to use this paper ?</a></li><li><a href="#bibtex" class="table-of-contents__link">Bibtex</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2021 Pierre Nodet.</div></div></div></footer></div>
<script src="/notes/styles.36e43167.js"></script>
<script src="/notes/runtime~main.9fb735cd.js"></script>
<script src="/notes/main.6c708b17.js"></script>
<script src="/notes/1.e087ac29.js"></script>
<script src="/notes/2.af5c63dd.js"></script>
<script src="/notes/ccc49370.852fedf5.js"></script>
<script src="/notes/a344bd16.f91d91f1.js"></script>
<script src="/notes/0c87fc9f.44bf6fde.js"></script>
</body>
</html>