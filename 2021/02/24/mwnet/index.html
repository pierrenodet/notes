<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/notes/rss.xml" title="notes Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/notes/atom.xml" title="notes Blog Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"><title data-react-helmet="true">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet) | notes</title><meta data-react-helmet="true" property="og:title" content="Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet) | notes"><meta data-react-helmet="true" name="description" content="MWNet"><meta data-react-helmet="true" property="og:description" content="MWNet"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><link data-react-helmet="true" rel="shortcut icon" href="/notes/img/favicon.ico"><link rel="stylesheet" href="/notes/styles.58026286.css">
<link rel="preload" href="/notes/styles.36e43167.js" as="script">
<link rel="preload" href="/notes/runtime~main.df64f7f1.js" as="script">
<link rel="preload" href="/notes/main.32211599.js" as="script">
<link rel="preload" href="/notes/1.e087ac29.js" as="script">
<link rel="preload" href="/notes/2.af5c63dd.js" as="script">
<link rel="preload" href="/notes/ccc49370.852fedf5.js" as="script">
<link rel="preload" href="/notes/a344bd16.f91d91f1.js" as="script">
<link rel="preload" href="/notes/17393f89.ecbcf9e0.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/notes/"><strong class="navbar__title">notes</strong></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/pierrenodet/notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/notes/"><strong class="navbar__title">notes</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a href="https://github.com/pierrenodet/notes" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper blog-wrapper"><div class="container margin-vert--lg"><div class="row"><div class="col col--2"><div class="sidebar_SWld thin-scrollbar"><h3 class="sidebarItemTitle_Km2m">Recent posts</h3><ul class="sidebarItemList_3UpA"><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/03/01/mentornet">MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels (MentorNet)</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/02/26/coteaching">Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching)</a></li><li class="sidebarItem_2T0D"><a aria-current="page" class="sidebarItemLink_v5H9 sidebarItemLinkActive_1anX" href="/notes/2021/02/24/mwnet">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/02/19/mlc">Meta Label Correction for Noisy Label Learning (MLC)</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2020/02/29/mutual-information">Flexible Biquality Learning with Mutual Information</a></li></ul></div></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_3-lP">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)</h1><div class="margin-vert--md"><time datetime="2021-02-24T00:00:00.000Z" class="blogPostDate_Ta7i">February 24, 2021  · 3 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"></div></div></header><section class="markdown"><p align="center"></p><p><img alt="MWNet" src="/notes/assets/images/mwnet-1161c0a8f3032e93b9bdfbbf809277d4.png"></p><p></p><ul><li><strong>code</strong> : <a href="https://github.com/xjtushujun/meta-weight-net" target="_blank" rel="noopener noreferrer">https://github.com/xjtushujun/meta-weight-net</a></li><li><strong>pdf</strong> : <a href="https://papers.nips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf" target="_blank" rel="noopener noreferrer">https://papers.nips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf</a></li></ul><ol><li>MWNet is an actual biquality learning algorithm (uses both trusted and untrusted data).</li><li>Meta Learning algorithm to reweight untrusted samples by learning a reweighted loss function (weighting function with sample loss as an input and a reweight sample loss as the output).</li><li>The meta model (an MLP) estimate the reweighting function while the main model learned with the reweighted function.</li><li>Both models are jointly trained by solving bilevel optimization. The meta model is learned by how much the main model performs on trusted labels. The main model is learned by how much it performs on the loss reweighted by the meta model.</li><li>Theoritical proofs on convergence of the algorithm.</li><li>Extensive experiments against a lot of State-of-the-Art competitors</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="assets"></a>Assets<a class="hash-link" href="#assets" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="strengths"></a>Strengths<a class="hash-link" href="#strengths" title="Direct link to heading">#</a></h3><ol><li>Small meta model (MLP) completly interpretable, as it&#x27;s a simple reweighting function (plottable on the loss domain).</li><li>Sound paper and some details on how and why the method works.</li><li>No over the top algorithm complexity.</li><li>Works well on assymetric noise (flip noise) while it&#x27;s a reweighting method.</li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="drawbacks"></a>Drawbacks<a class="hash-link" href="#drawbacks" title="Direct link to heading">#</a></h3><ol><li>No guarantees for the algorithm to work on Not At Random noise, as the meta model only uses loss values as an input to reweight it and no experiments have been conducted.</li><li>Meta Learning algorithm (can be tricky/hard to implement for a wide spread of neural network architectures and optimizers). Thanksfully some library solve this problem (see higher for PyTorch).</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="novelty"></a>Novelty<a class="hash-link" href="#novelty" title="Direct link to heading">#</a></h2><p>The approach is novel, it reuses the idea from L2RW (using meta learning to reweight samples), but instead of learning the weight of every samples, it smarty learns how to reweight the base loss.</p><p align="center"></p><p><img alt="MWNetLoss" src="/notes/assets/images/mwnet-loss-71a614ecfcb00d5a9c12d250f29de667.png"></p><p></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="significance"></a>Significance<a class="hash-link" href="#significance" title="Direct link to heading">#</a></h2><p>Significant as a lot of results (detailed) show improvements against SotA, altough not that high against GLC a much simpler and sound algorithm. Theoritcal proofs are quite important for the field and have been reused by a lot of downstreams papers. </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="soudness"></a>Soudness<a class="hash-link" href="#soudness" title="Direct link to heading">#</a></h2><p>Technically sound, proofs of main theorems proving convergence of the algorithm have been provided.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="evaluation"></a>Evaluation<a class="hash-link" href="#evaluation" title="Direct link to heading">#</a></h2><p>Sufficient evaluation of the algorithm have been provided.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-can-i-useenhance-the-paper-"></a>How can I use/enhance the paper ?<a class="hash-link" href="#how-can-i-useenhance-the-paper-" title="Direct link to heading">#</a></h2><ol><li>I could enhance the paper by adapting it to Not At Random Noise.</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="what-did-i-learn-from-reading-this-paper-"></a>What did I learn from reading this paper ?<a class="hash-link" href="#what-did-i-learn-from-reading-this-paper-" title="Direct link to heading">#</a></h2><p>Deeper understanding of meta learning (how to do bilevel optimization schemes).</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="new-paper-to-readinteresting-citations"></a>New paper to read/Interesting Citations<a class="hash-link" href="#new-paper-to-readinteresting-citations" title="Direct link to heading">#</a></h2><ol><li>Joint optimization framework for learning with noisy labels (CPVR 2018)</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-i-am-going-to-use-this-paper-"></a>How I am going to use this paper ?<a class="hash-link" href="#how-i-am-going-to-use-this-paper-" title="Direct link to heading">#</a></h2><p>To Implement</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="bibtex"></a>Bibtex<a class="hash-link" href="#bibtex" title="Direct link to heading">#</a></h2><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{NEURIPS2019_e58cc5ca,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> booktitle = {Advances in Neural Information Processing Systems},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\&#x27;{e}-Buc and E. Fox and R. Garnett},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> pages = {},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> publisher = {Curran Associates, Inc.},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> title = {Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> url = {https://proceedings.neurips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> volume = {32},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> year = {2019}</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">}</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/notes/tags/reading-notes">reading-notes</a><a class="margin-horiz--sm" href="/notes/tags/meta-learning">meta-learning</a><a class="margin-horiz--sm" href="/notes/tags/importance-reweighting">importance-reweighting</a></div></footer></article><div><a href="https://github.com/pierrenodet/notes/edit/master/notes/2021-02-24-mwnet.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2LL7"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="margin-vert--xl"><nav class="pagination-nav" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/notes/2021/02/26/coteaching"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching)</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/notes/2021/02/19/mlc"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Meta Label Correction for Noisy Label Learning (MLC) »</div></a></div></nav></div></main><div class="col col--2"><div class="tableOfContents_2xL- thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#assets" class="table-of-contents__link">Assets</a><ul><li><a href="#strengths" class="table-of-contents__link">Strengths</a></li><li><a href="#drawbacks" class="table-of-contents__link">Drawbacks</a></li></ul></li><li><a href="#novelty" class="table-of-contents__link">Novelty</a></li><li><a href="#significance" class="table-of-contents__link">Significance</a></li><li><a href="#soudness" class="table-of-contents__link">Soudness</a></li><li><a href="#evaluation" class="table-of-contents__link">Evaluation</a></li><li><a href="#how-can-i-useenhance-the-paper-" class="table-of-contents__link">How can I use/enhance the paper ?</a></li><li><a href="#what-did-i-learn-from-reading-this-paper-" class="table-of-contents__link">What did I learn from reading this paper ?</a></li><li><a href="#new-paper-to-readinteresting-citations" class="table-of-contents__link">New paper to read/Interesting Citations</a></li><li><a href="#how-i-am-going-to-use-this-paper-" class="table-of-contents__link">How I am going to use this paper ?</a></li><li><a href="#bibtex" class="table-of-contents__link">Bibtex</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2021 Pierre Nodet.</div></div></div></footer></div>
<script src="/notes/styles.36e43167.js"></script>
<script src="/notes/runtime~main.df64f7f1.js"></script>
<script src="/notes/main.32211599.js"></script>
<script src="/notes/1.e087ac29.js"></script>
<script src="/notes/2.af5c63dd.js"></script>
<script src="/notes/ccc49370.852fedf5.js"></script>
<script src="/notes/a344bd16.f91d91f1.js"></script>
<script src="/notes/17393f89.ecbcf9e0.js"></script>
</body>
</html>