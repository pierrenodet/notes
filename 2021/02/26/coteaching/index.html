<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/notes/rss.xml" title="notes Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/notes/atom.xml" title="notes Blog Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"><title data-react-helmet="true">Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching) | notes</title><meta data-react-helmet="true" property="og:title" content="Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching) | notes"><meta data-react-helmet="true" name="description" content="Co-teaching"><meta data-react-helmet="true" property="og:description" content="Co-teaching"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><link data-react-helmet="true" rel="shortcut icon" href="/notes/img/favicon.ico"><link rel="stylesheet" href="/notes/styles.58026286.css">
<link rel="preload" href="/notes/styles.1257a86e.js" as="script">
<link rel="preload" href="/notes/runtime~main.62fca182.js" as="script">
<link rel="preload" href="/notes/main.a30aff7b.js" as="script">
<link rel="preload" href="/notes/1.725471b9.js" as="script">
<link rel="preload" href="/notes/2.1fde710c.js" as="script">
<link rel="preload" href="/notes/ccc49370.49403214.js" as="script">
<link rel="preload" href="/notes/a344bd16.2c927848.js" as="script">
<link rel="preload" href="/notes/ee19a400.1e2f4499.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/notes/"><strong class="navbar__title">notes</strong></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/pierrenodet/notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/notes/"><strong class="navbar__title">notes</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a href="https://github.com/pierrenodet/notes" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper blog-wrapper"><div class="container margin-vert--lg"><div class="row"><div class="col col--2"><div class="sidebar_SWld thin-scrollbar"><h3 class="sidebarItemTitle_Km2m">Recent posts</h3><ul class="sidebarItemList_3UpA"><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/03/21/mta">Meta Transition Adaptation for Robust Deep Learning with Noisy Labels</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/03/01/mentornet">MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels (MentorNet)</a></li><li class="sidebarItem_2T0D"><a aria-current="page" class="sidebarItemLink_v5H9 sidebarItemLinkActive_1anX" href="/notes/2021/02/26/coteaching">Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching)</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/02/24/mwnet">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/notes/2021/02/19/mlc">Meta Label Correction for Noisy Label Learning (MLC)</a></li></ul></div></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_3-lP">Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching)</h1><div class="margin-vert--md"><time datetime="2021-02-26T00:00:00.000Z" class="blogPostDate_Ta7i">February 26, 2021  Â· 3 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"></div></div></header><section class="markdown"><p align="center"></p><p><img alt="Co-teaching" src="/notes/assets/images/coteaching-8a41dcede948608101271068a30bcbdc.png"></p><p></p><ul><li><strong>code</strong> : <a href="https://github.com/bhanML/Co-teaching" target="_blank" rel="noopener noreferrer">https://github.com/bhanML/Co-teaching</a></li><li><strong>pdf</strong> : <a href="https://arxiv.org/pdf/1804.06872.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1804.06872.pdf</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><ol><li>Collaborative algorithm between two networks to learn on untrusted data only.</li><li>Each round, loss of each network is computed on it&#x27;s own minibatch. The samples with the smallest loss are considered the most informative and are given to the other network to learn (link to curriculum learning)</li><li>Leverage memorization effect of deep neural networks (first learn clean and easy patterns then overfit noise).</li><li>Introduction of a dynamic size for the number of informative samples used (start high then reduce with number of epochs)</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="assets"></a>Assets<a class="hash-link" href="#assets" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="strengths"></a>Strengths<a class="hash-link" href="#strengths" title="Direct link to heading">#</a></h3><ol><li>Leverage the memorization effect</li><li>Collaborative learning between two models</li><li>Small loss instances are the most informatives.</li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="drawbacks"></a>Drawbacks<a class="hash-link" href="#drawbacks" title="Direct link to heading">#</a></h3><ol><li>Needs noise rate for hyperparameter tuning (can be fixed in the biquality case).</li><li>Tested on only three vision datasets and two noise types.</li><li>An empirical paper (no proofs) with good improvments on pair noise but not on symmetric one.</li><li>Somewhat high algorithm complexity as it needs two learn two networks at once.</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="novelty"></a>Novelty<a class="hash-link" href="#novelty" title="Direct link to heading">#</a></h2><p>It&#x27;s a novel algorithm as it reuses proven idea from different fields, small loss instances as informative from Mentor Net (distillation), collaborative learning from Co-Training (semi-supervised learning) and the memorization effect of neural networks (robust learninng to label noise).</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="significance"></a>Significance<a class="hash-link" href="#significance" title="Direct link to heading">#</a></h2><p>The results are somewhat significant, especially on pair noise. Nonetheless too few datasets and kind of noises have been used for an experimental paper.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="soudness"></a>Soudness<a class="hash-link" href="#soudness" title="Direct link to heading">#</a></h2><p>The paper seems OK but no proofs have been provided but convergence seems to work empirically.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="evaluation"></a>Evaluation<a class="hash-link" href="#evaluation" title="Direct link to heading">#</a></h2><p>The evaluation is somewhat weak because of the lack of proofs and lack of more in depth experiments.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-can-i-useenhance-the-paper-"></a>How can I use/enhance the paper ?<a class="hash-link" href="#how-can-i-useenhance-the-paper-" title="Direct link to heading">#</a></h2><p>The mixing of ideas from different fields to assemble an algorithm is quite nice and I should reuse the free main principles about memorization, two networks and small loss instances.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="what-did-i-learn-from-reading-this-paper-"></a>What did I learn from reading this paper ?<a class="hash-link" href="#what-did-i-learn-from-reading-this-paper-" title="Direct link to heading">#</a></h2><p>That the three principles above works well empirically.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="new-paper-to-readinteresting-citations"></a>New paper to read/Interesting Citations<a class="hash-link" href="#new-paper-to-readinteresting-citations" title="Direct link to heading">#</a></h2><ol><li>Mentor Net</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-i-am-going-to-use-this-paper-"></a>How I am going to use this paper ?<a class="hash-link" href="#how-i-am-going-to-use-this-paper-" title="Direct link to heading">#</a></h2><p>Citation. The requirement of the noise rate could make it part of the biquality learning algorithm family but i&#x27;m not quite sure yet. Otherwise Implement it.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="bibtex"></a>Bibtex<a class="hash-link" href="#bibtex" title="Direct link to heading">#</a></h2><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{NEURIPS2018_a19744e2,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> author = {Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> booktitle = {Advances in Neural Information Processing Systems},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> pages = {},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> publisher = {Curran Associates, Inc.},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> title = {Co-teaching: Robust training of deep neural networks with extremely noisy labels},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> url = {https://proceedings.neurips.cc/paper/2018/file/a19744e268754fb0148b017647355b7b-Paper.pdf},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> volume = {31},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"> year = {2018}</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">}</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/notes/tags/reading-notes">reading-notes</a><a class="margin-horiz--sm" href="/notes/tags/instance-selection">instance-selection</a><a class="margin-horiz--sm" href="/notes/tags/collaborative-learning">collaborative-learning</a></div></footer></article><div><a href="https://github.com/pierrenodet/notes/edit/master/notes/2021-02-26-coteaching.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2LL7"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="margin-vert--xl"><nav class="pagination-nav" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/notes/2021/03/01/mentornet"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Â« MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels (MentorNet)</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/notes/2021/02/24/mwnet"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet) Â»</div></a></div></nav></div></main><div class="col col--2"><div class="tableOfContents_2xL- thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link">Summary</a></li><li><a href="#assets" class="table-of-contents__link">Assets</a><ul><li><a href="#strengths" class="table-of-contents__link">Strengths</a></li><li><a href="#drawbacks" class="table-of-contents__link">Drawbacks</a></li></ul></li><li><a href="#novelty" class="table-of-contents__link">Novelty</a></li><li><a href="#significance" class="table-of-contents__link">Significance</a></li><li><a href="#soudness" class="table-of-contents__link">Soudness</a></li><li><a href="#evaluation" class="table-of-contents__link">Evaluation</a></li><li><a href="#how-can-i-useenhance-the-paper-" class="table-of-contents__link">How can I use/enhance the paper ?</a></li><li><a href="#what-did-i-learn-from-reading-this-paper-" class="table-of-contents__link">What did I learn from reading this paper ?</a></li><li><a href="#new-paper-to-readinteresting-citations" class="table-of-contents__link">New paper to read/Interesting Citations</a></li><li><a href="#how-i-am-going-to-use-this-paper-" class="table-of-contents__link">How I am going to use this paper ?</a></li><li><a href="#bibtex" class="table-of-contents__link">Bibtex</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2021 Pierre Nodet.</div></div></div></footer></div>
<script src="/notes/styles.1257a86e.js"></script>
<script src="/notes/runtime~main.62fca182.js"></script>
<script src="/notes/main.a30aff7b.js"></script>
<script src="/notes/1.725471b9.js"></script>
<script src="/notes/2.1fde710c.js"></script>
<script src="/notes/ccc49370.49403214.js"></script>
<script src="/notes/a344bd16.2c927848.js"></script>
<script src="/notes/ee19a400.1e2f4499.js"></script>
</body>
</html>