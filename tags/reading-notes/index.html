<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.72">
<link rel="alternate" type="application/rss+xml" href="/notes/rss.xml" title="notes Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/notes/atom.xml" title="notes Blog Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"><title data-react-helmet="true">Posts tagged &quot;reading-notes&quot; | notes</title><meta data-react-helmet="true" property="og:title" content="Posts tagged &quot;reading-notes&quot; | notes"><meta data-react-helmet="true" name="description" content="Blog | Tagged &quot;reading-notes&quot;"><meta data-react-helmet="true" property="og:description" content="Blog | Tagged &quot;reading-notes&quot;"><meta data-react-helmet="true" property="og:url" content="https://pierrenodet.github.io/notes/tags/reading-notes"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/notes/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://pierrenodet.github.io/notes/tags/reading-notes"><link data-react-helmet="true" rel="alternate" href="https://pierrenodet.github.io/notes/tags/reading-notes" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://pierrenodet.github.io/notes/tags/reading-notes" hreflang="x-default"><link rel="stylesheet" href="/notes/assets/css/styles.31542d14.css">
<link rel="preload" href="/notes/assets/js/styles.0f00533d.js" as="script">
<link rel="preload" href="/notes/assets/js/runtime~main.64954c49.js" as="script">
<link rel="preload" href="/notes/assets/js/main.7f94ea38.js" as="script">
<link rel="preload" href="/notes/assets/js/1.6bd91039.js" as="script">
<link rel="preload" href="/notes/assets/js/2.2283bb1b.js" as="script">
<link rel="preload" href="/notes/assets/js/6875c492.b040c515.js" as="script">
<link rel="preload" href="/notes/assets/js/a344bd16.c4ace5bd.js" as="script">
<link rel="preload" href="/notes/assets/js/5ef9ebc0.69ddb0da.js" as="script">
<link rel="preload" href="/notes/assets/js/f3aff117.d1623f58.js" as="script">
<link rel="preload" href="/notes/assets/js/b5cba463.7f750219.js" as="script">
<link rel="preload" href="/notes/assets/js/5d5155db.e39b7182.js" as="script">
<link rel="preload" href="/notes/assets/js/8256a383.d7cbc7c3.js" as="script">
<link rel="preload" href="/notes/assets/js/01b05ba1.a2557162.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#main" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle" type="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/notes/"><strong class="navbar__title">notes</strong></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/pierrenodet/notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/notes/"><strong class="navbar__title">notes</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a href="https://github.com/pierrenodet/notes" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper blog-wrapper"><div class="container margin-vert--lg"><div class="row"><div class="col col--3"><div class="sidebar_2ahu thin-scrollbar"><h3 class="sidebarItemTitle_2hhb">Recent posts</h3><ul class="sidebarItemList_2xAf"><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/notes/2021/03/25/mslc">Learning to Purify Noisy Labels via Meta Soft Label Corrector</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/notes/2021/03/21/mta">Meta Transition Adaptation for Robust Deep Learning with Noisy Labels</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/notes/2021/03/01/mentornet">MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels (MentorNet)</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/notes/2021/02/26/coteaching">Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching)</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/notes/2021/02/24/mwnet">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)</a></li></ul></div></div><main class="col col--7"><h1>5 posts tagged with &quot;reading-notes&quot;</h1><a href="/notes/tags">View All Tags</a><div class="margin-vert--xl"><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_GeHD"><a href="/notes/2021/03/01/mentornet">MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels (MentorNet)</a></h2><div class="margin-vert--md"><time datetime="2021-03-01T00:00:00.000Z" class="blogPostDate_fNvV">March 1, 2021 · 3 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"></div></div></header><div class="markdown"><p align="center"></p><p><img alt="MentorNet" src="/notes/assets/images/mentornet-842f22abdfffc0556dce88947b84b619.gif"></p><p></p><ul><li><strong>code</strong> : <a href="https://github.com/google/mentornet" target="_blank" rel="noopener noreferrer">https://github.com/google/mentornet</a></li><li><strong>pdf</strong> : <a href="https://arxiv.org/pdf/1712.05055.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1712.05055.pdf</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><ol><li>Modern take/refresh on curriculum learning</li><li>Meta Learning algorithm to learn a curriculum from data instead of hand designing it : Learning curriculum</li><li>Actual Biquality Learning algorithm (learning the curriculum on a mix of trusted and untrusted data)</li><li>Proposed architecture of MentorNet (model that represent the learned curriculum) with bidirection LSTM (for loss evolution and memory) and MLP</li><li>MentorNet learned as a supervised classification task (clean samples as 1, corrupted samples as 0). Student Net learn the usual tasks on reweighted samples from MentorNet.</li><li>Insights on the algorithm that might (not automatically) optimize an M-Robust loss function.</li><li>Exaustive experiments against state of the art curriculum based reweighting scheme.</li></ol></div><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/notes/tags/reading-notes">reading-notes</a><a class="margin-horiz--sm" href="/notes/tags/meta-learning">meta-learning</a><a class="margin-horiz--sm" href="/notes/tags/importance-reweighting">importance-reweighting</a></div><div class="col text--right"><a aria-label="Read more about MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels (MentorNet)" href="/notes/2021/03/01/mentornet"><strong>Read More</strong></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_GeHD"><a href="/notes/2021/02/26/coteaching">Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching)</a></h2><div class="margin-vert--md"><time datetime="2021-02-26T00:00:00.000Z" class="blogPostDate_fNvV">February 26, 2021 · 3 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"></div></div></header><div class="markdown"><p align="center"></p><p><img alt="Co-teaching" src="/notes/assets/images/coteaching-8a41dcede948608101271068a30bcbdc.png"></p><p></p><ul><li><strong>code</strong> : <a href="https://github.com/bhanML/Co-teaching" target="_blank" rel="noopener noreferrer">https://github.com/bhanML/Co-teaching</a></li><li><strong>pdf</strong> : <a href="https://arxiv.org/pdf/1804.06872.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1804.06872.pdf</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><ol><li>Collaborative algorithm between two networks to learn on untrusted data only.</li><li>Each round, loss of each network is computed on it&#x27;s own minibatch. The samples with the smallest loss are considered the most informative and are given to the other network to learn (link to curriculum learning)</li><li>Leverage memorization effect of deep neural networks (first learn clean and easy patterns then overfit noise).</li><li>Introduction of a dynamic size for the number of informative samples used (start high then reduce with number of epochs)</li></ol></div><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/notes/tags/reading-notes">reading-notes</a><a class="margin-horiz--sm" href="/notes/tags/instance-selection">instance-selection</a><a class="margin-horiz--sm" href="/notes/tags/collaborative-learning">collaborative-learning</a></div><div class="col text--right"><a aria-label="Read more about Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels (Co-teaching)" href="/notes/2021/02/26/coteaching"><strong>Read More</strong></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_GeHD"><a href="/notes/2021/02/24/mwnet">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)</a></h2><div class="margin-vert--md"><time datetime="2021-02-24T00:00:00.000Z" class="blogPostDate_fNvV">February 24, 2021 · 3 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"></div></div></header><div class="markdown"><p align="center"></p><p><img alt="MWNet" src="/notes/assets/images/mwnet-1161c0a8f3032e93b9bdfbbf809277d4.png"></p><p></p><ul><li><strong>code</strong> : <a href="https://github.com/xjtushujun/meta-weight-net" target="_blank" rel="noopener noreferrer">https://github.com/xjtushujun/meta-weight-net</a></li><li><strong>pdf</strong> : <a href="https://papers.nips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf" target="_blank" rel="noopener noreferrer">https://papers.nips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><ol><li>MWNet is an actual biquality learning algorithm (uses both trusted and untrusted data).</li><li>Meta Learning algorithm to reweight untrusted samples by learning a reweighted loss function (weighting function with sample loss as an input and a reweight sample loss as the output) : Learning curriculum</li><li>The meta model (an MLP) estimate the reweighting function while the main model learned with the reweighted function.</li><li>Both models are jointly trained by solving bilevel optimization. The meta model is learned by how much the main model performs on trusted labels. The main model is learned by how much it performs on the loss reweighted by the meta model.</li><li>Theoritical proofs on convergence of the algorithm.</li><li>Extensive experiments against a lot of State-of-the-Art competitors</li></ol></div><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/notes/tags/reading-notes">reading-notes</a><a class="margin-horiz--sm" href="/notes/tags/meta-learning">meta-learning</a><a class="margin-horiz--sm" href="/notes/tags/importance-reweighting">importance-reweighting</a></div><div class="col text--right"><a aria-label="Read more about Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)" href="/notes/2021/02/24/mwnet"><strong>Read More</strong></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_GeHD"><a href="/notes/2021/02/19/mlc">Meta Label Correction for Noisy Label Learning (MLC)</a></h2><div class="margin-vert--md"><time datetime="2021-02-19T00:00:00.000Z" class="blogPostDate_fNvV">February 19, 2021 · 4 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"></div></div></header><div class="markdown"><p align="center"></p><p><img alt="MLC" src="/notes/assets/images/mlc-c349b4ef10e45681ad6f2384831d06ee.png"></p><p></p><ul><li><strong>code</strong> : <a href="https://github.com/microsoft/MLC" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/MLC</a></li><li><strong>pdf</strong> : <a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/12/mlc_aaai21_zheng_camera.pdf" target="_blank" rel="noopener noreferrer">https://www.microsoft.com/en-us/research/uploads/prod/2020/12/mlc_aaai21_zheng_camera.pdf</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="summary"></a>Summary<a class="hash-link" href="#summary" title="Direct link to heading">#</a></h2><ol><li>MLC is an actual biquality learning algorithm (uses both trusted and untrusted data).</li><li>MLC uses a Meta Learning approach to correct corrupted/untrusted labels : Learning to correct.</li><li>The meta model (a label correction network) correct label for corrupted instances while the main model learns on corrected labels.</li><li>Both models are jointly trained by solving bilevel optimization. The meta model is learned by how much the main model performs on trusted labels. The main model is learned by how much it performs on labels corrected by the meta model.</li><li>A (novel) k step look ahead SGD to solve this optimization problem.</li><li>Meta Model has an embedding of the data and the untrusted label as an input to learn how to correct it.</li><li>Experiments on widely used datasets and noises against State-of-the-Art competitors.</li></ol></div><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/notes/tags/reading-notes">reading-notes</a><a class="margin-horiz--sm" href="/notes/tags/meta-learning">meta-learning</a><a class="margin-horiz--sm" href="/notes/tags/label-correction">label-correction</a></div><div class="col text--right"><a aria-label="Read more about Meta Label Correction for Noisy Label Learning (MLC)" href="/notes/2021/02/19/mlc"><strong>Read More</strong></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_GeHD"><a href="/notes/2020/01/11/proofs">Some proofs about Statistical Learning and Label Noise</a></h2><div class="margin-vert--md"><time datetime="2020-01-11T00:00:00.000Z" class="blogPostDate_fNvV">January 11, 2020 · 4 min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"></div></div></header><div class="markdown"><p>When doing binary classfication, having noise in our label means that they can be flipped from one class to the other. When observing a label, we will never know if it has been flipped or not, only the probability of the flip to occur.</p><p>Noisy labels can definitly mess up with model accuracy and make us unable to evaluate it. For example, answering this question can be quite hard : Is it a false positive or the algorithm was right and the label was flipped ?</p><p>Thanksfully researchers are clever and they found some theoritical guarantees to still be able to do machine learning in this setup. We will redo proofs found in various papers covering this subject, starting from the easiest setup (random noise) to the hardest one (individual dependent noise).</p></div><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/notes/tags/reading-notes">reading-notes</a><a class="margin-horiz--sm" href="/notes/tags/importance-reweighting">importance-reweighting</a></div><div class="col text--right"><a aria-label="Read more about Some proofs about Statistical Learning and Label Noise" href="/notes/2020/01/11/proofs"><strong>Read More</strong></a></div></footer></article></div></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2021 Pierre Nodet.</div></div></div></footer></div>
<script src="/notes/assets/js/styles.0f00533d.js"></script>
<script src="/notes/assets/js/runtime~main.64954c49.js"></script>
<script src="/notes/assets/js/main.7f94ea38.js"></script>
<script src="/notes/assets/js/1.6bd91039.js"></script>
<script src="/notes/assets/js/2.2283bb1b.js"></script>
<script src="/notes/assets/js/6875c492.b040c515.js"></script>
<script src="/notes/assets/js/a344bd16.c4ace5bd.js"></script>
<script src="/notes/assets/js/5ef9ebc0.69ddb0da.js"></script>
<script src="/notes/assets/js/f3aff117.d1623f58.js"></script>
<script src="/notes/assets/js/b5cba463.7f750219.js"></script>
<script src="/notes/assets/js/5d5155db.e39b7182.js"></script>
<script src="/notes/assets/js/8256a383.d7cbc7c3.js"></script>
<script src="/notes/assets/js/01b05ba1.a2557162.js"></script>
</body>
</html>