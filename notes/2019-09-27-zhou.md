---
title: Zhou - A Brief Introduction to Weakly Supervised Learning
---

## Introduction

Cet article est une survey sur l'apprentissage faiblement supervisé.

Trois types d'apprentissage faiblement supervisé sont décris :

1. L'apprentissage supervisé incomplet
2. L'apprentissage supervisé inexact
3. L'apprentissage supervisé imprécis

## L'apprentissage incomplet

L'apprentissage incomplet correspond au cas où seulement une partie des données d'apprentissage sont labélisées.

Il peut se découper en deux sous catégories d'apprentissage si un oracle est disponible pour demander des étiquettes supplémentaires au cours de la tache d'apprentissage.

Si l'oracle est disponible, alors on parlera plus d'apprentissage actif où l'objectif sera d'optimiser le nombre de requête faite à l'expert tout en atteignant la meilleur précision possible.

S'il ne l'est pas, alors plusieurs possibilités s'offre à nous. L'apprentissage semi-supervisé essaiera d'utiliser le plus possible les données non labélisées pour améliorer les performances du modèle.

### L'apprentissage actif

L'apprentissage supervisé classique est dit passif car les étiquettes sont données passivement à l'àlgorithme d'apprentissage au avant l'apprentissage. Avec l'apprentissage actif, l'algorithm va lui même demander à un oracle les étiquettes des individus qui vont lui paraitre le plus important selon un critère.

Il existe principalement deux types de critères, informatif et representatif.

Les crtières informatifs choissisent les individus qui vont le mieux réduire l'incertitude du modèle, mais ceux-ci sont très dépendants des premiers individus misent à disposition.

Les critères représentatifs prennent les individus qui améliore le mieux la réprensatation des données que se fait l'algorithme.

Les critères informatifs recherchent l'exploitation et les critères représentatifs visent l'exploration.

Une piste d'amélioration de l'apprentissage actif serait de concevoir des stratégies qui sont capables de s'adapter en fonction des données à choisir à chaque étape d'explorer ou d'exploiter les données.

### L'apprentissage semi-supervisé

L'apprentissage semi-supervisé en plus de l'apprentissage supervisé utilise les données non étiquetées du dataset d'apprentissage pour par example estimer la distribution des données.

En apprentissage semi-supervisé, deux hypothèses sont souvent posées. La première suppose que les données forment naturellement des clusters, où les individus d'un même cluster possèdent la même étiquette (hypothèse de cluster). La seconde suppose que deux individus proches possèdent des prédictions proches (hypothèse de manifold).

Il y a quatres grandes catégories de méthodes semi-supervisé :
* Les méthodes génératives
* Les méthodes basées sur des graphes
* Les méthodes de séparation de faible densité
* Les méthodes basées sur des désacords.

#### Méthodes génératives

Les méthodes génératives supposent que les données étiquetées et non étiquetées sont issues du même processus génératif. Ainsi les données non étiquetées peuvent être traitées comme des valeurs manquantes des paramètres du modèle génératif.

La principale difficulté de cette méthode est d'avoir suffisament de connaisance métier sur les données pour choisir la bonne famille de modèle génératif.

#### Méthodes graphes

Les méthodes basées sur des graphes ont pour but de construire un graphe où les sommets sont les individus et les arêtes sont les relations entre eux construite par une mesure de similarité. Les labels sont propagés dans le graphe à partir de ceux disponibles via un critère de propagation.

Leurs principaux défauts sont d'être très couteux en stockage et en CPU à cause de la représentation des données (tables vs graphes), transductifs car compliqué de reconstruire le graphe pour de nouveaux individus, et très dépendants de la mesure de similarité et du critère de propagation.

#### Méthodes séparateurs

Les S3VM (adaptation des SVM au semi-supervisé) prennent en compte les données non étiquetées pour choisir un meilleur séparateur passant par les régions les moins denses.

Ici les données non étiquetées sont prises en compte en mettant une "fausse" étiquette, mais cela rend le problème d'optimisation plus complexe.

#### Méthodes désacords

Les méthodes basées sur des désacords utilisent la coopération entre plusieurs modèles.

La méthode la plus connue est le co-training. Elle consiste à entraîner plusieurs classifieurs (souvent deux), chacun basé sur une vue du corpus, puis à les améliorer entre eux à l’aide d’une masse importante de données non annotées. Les classifieurs plus à même de classer un exemple donné jouant le rôle de "professeurs" pour les autres.

Ces méthodes permettent naturellement d'allier apprentissage actif et apprentissage semi-supervisé.

En revanche, si les vues du corpus et donc les classifieurs sont dépendants entre eux, le co-training ne permettera pas d'améliorer les performances finales des modèles.

#### Variante transductive

Il existe une variante transductive de l'apprentissage semi-supervisé où l'on ne cherche pas à généraliser à d'autres données mais à être le plus performant sur un jeu de données définit à l'avance.

Cela peut s'appliquer à l'imputation de données manquantes en concevant un modèle à partir des données présentes et à une mesure de similarité entre individus.

## L'apprentissage inexact

## L'apprentissage imprécis
