<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="generator" content="Docusaurus">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">

<title data-react-helmet="true">Zhou - A Brief Introduction to Weakly Supervised Learning</title>

<meta data-react-helmet="true" http-equiv="x-ua-compatible" content="ie=edge"><meta data-react-helmet="true" property="og:title" content="Zhou - A Brief Introduction to Weakly Supervised Learning"><meta data-react-helmet="true" name="description" content="## Introduction"><meta data-react-helmet="true" property="og:description" content="## Introduction"><meta data-react-helmet="true" name="twitter:card" content="summary">

<link data-react-helmet="true" rel="shortcut icon" href="/thesis/img/favicon.ico">


<link rel="stylesheet" href="/thesis/styles.809cbcd8.css">

</head>
<body>

<div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a aria-current="page" class="navbar__brand active" href="/thesis/"><img class="navbar__logo" src="/thesis/img/logo.png" alt="thesis logo"><strong>thesis</strong></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/thesis/manuscript/introduction">Manuscript</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/thesis/notes">Reading Notes</a><a class="navbar__item navbar__link" target="_blank" rel="noopener noreferrer" href="https://github.com/pierrenodet/thesis">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a aria-current="page" class="navbar__brand active" href="/thesis/"><img class="navbar__logo" src="/thesis/img/logo.png" alt="thesis logo"><strong>thesis</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/thesis/manuscript/introduction">Manuscript</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/thesis/notes">Reading Notes</a></li><li class="menu__list-item"><a class="menu__link" target="_blank" rel="noopener noreferrer" href="https://github.com/pierrenodet/thesis">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="container margin-vert--xl"><div class="row"><div class="col col--8 col--offset-2"><article><header><h1 class="margin-bottom--sm blogPostTitle_2RZH">Zhou - A Brief Introduction to Weakly Supervised Learning</h1><div class="margin-bottom--sm"><time datetime="2019-09-27T00:00:00.000Z" class="blogPostDate_3tRe">September 27, 2019</time></div><div class="avatar margin-bottom--md"><div class="avatar__intro"></div></div></header><section class="markdown"><h2><a aria-hidden="true" tabindex="-1" class="anchor" id="introduction"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#introduction" title="Direct link to heading">#</a>Introduction</h2><p>Cet article est une survey sur l&#x27;apprentissage faiblement supervisé.</p><p>Trois types d&#x27;apprentissage faiblement supervisés sont décrits :</p><ol><li>L&#x27;apprentissage supervisé incomplet</li><li>L&#x27;apprentissage supervisé inexact</li><li>L&#x27;apprentissage supervisé imprécis</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor" id="lapprentissage-incomplet"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#lapprentissage-incomplet" title="Direct link to heading">#</a>L&#x27;apprentissage incomplet</h2><p>L&#x27;apprentissage incomplet correspond au cas où seulement une partie des données d&#x27;apprentissage sont labélisées.</p><p>Il peut se découper en deux sous catégories d&#x27;apprentissage si un oracle est disponible pour demander des étiquettes supplémentaires au cours de la tache d&#x27;apprentissage.</p><p>Si l&#x27;oracle est disponible, alors on parlera plus d&#x27;apprentissage actif où l&#x27;objectif sera d&#x27;optimiser le nombre de requête faite à l&#x27;expert tout en atteignant la meilleur précision possible.</p><p>S&#x27;il ne l&#x27;est pas, alors plusieurs possibilités s&#x27;offre à nous. L&#x27;apprentissage semi-supervisé essaiera d&#x27;utiliser le plus possible les données non labélisées pour améliorer les performances du modèle.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor" id="lapprentissage-actif"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#lapprentissage-actif" title="Direct link to heading">#</a>L&#x27;apprentissage actif</h3><p>L&#x27;apprentissage supervisé classique est dit passif car les étiquettes sont données passivement à l&#x27;algorithme d&#x27;apprentissage au avant l&#x27;apprentissage. Avec l&#x27;apprentissage actif, l&#x27;algorithm va lui même demander à un oracle les étiquettes des individus qui vont lui paraitre le plus important selon un critère.</p><p>Il existe principalement deux types de critères, informatif et representatif.</p><p>Les crtières informatifs choissisent les individus qui vont le mieux réduire l&#x27;incertitude du modèle, mais ceux-ci sont très dépendants des premiers individus misent à disposition.</p><p>Les critères représentatifs prennent les individus qui améliore le mieux la réprensatation des données que se fait l&#x27;algorithme.</p><p>Les critères informatifs recherchent l&#x27;exploitation et les critères représentatifs visent l&#x27;exploration.</p><p>Une piste d&#x27;amélioration de l&#x27;apprentissage actif serait de concevoir des stratégies qui sont capables de s&#x27;adapter en fonction des données à choisir à chaque étape d&#x27;explorer ou d&#x27;exploiter les données.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor" id="lapprentissage-semi-supervisé"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#lapprentissage-semi-supervisé" title="Direct link to heading">#</a>L&#x27;apprentissage semi-supervisé</h3><p>L&#x27;apprentissage semi-supervisé en plus de l&#x27;apprentissage supervisé utilise les données non étiquetées du dataset d&#x27;apprentissage pour par example estimer la distribution des données.</p><p>En apprentissage semi-supervisé, deux hypothèses sont souvent posées. La première suppose que les données forment naturellement des clusters, où les individus d&#x27;un même cluster possèdent la même étiquette (hypothèse de cluster). La seconde suppose que deux individus proches possèdent des prédictions proches (hypothèse de manifold).</p><p>Il y a quatres grandes catégories de méthodes semi-supervisé :</p><ul><li>Les méthodes génératives</li><li>Les méthodes basées sur des graphes</li><li>Les méthodes de séparation de faible densité</li><li>Les méthodes basées sur des désacords.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor" id="méthodes-génératives"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#méthodes-génératives" title="Direct link to heading">#</a>Méthodes génératives</h4><p>Les méthodes génératives supposent que les données étiquetées et non étiquetées sont issues du même processus génératif. Ainsi les données non étiquetées peuvent être traitées comme des valeurs manquantes des paramètres du modèle génératif.</p><p>La principale difficulté de cette méthode est d&#x27;avoir suffisament de connaisance métier sur les données pour choisir la bonne famille de modèle génératif.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor" id="méthodes-graphes"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#méthodes-graphes" title="Direct link to heading">#</a>Méthodes graphes</h4><p>Les méthodes basées sur des graphes ont pour but de construire un graphe où les sommets sont les individus et les arêtes sont les relations entre eux construite par une mesure de similarité. Les labels sont propagés dans le graphe à partir de ceux disponibles via un critère de propagation.</p><p>Leurs principaux défauts sont d&#x27;être très couteux en stockage et en CPU à cause de la représentation des données (tables vs graphes), transductifs car compliqué de reconstruire le graphe pour de nouveaux individus, et très dépendants de la mesure de similarité et du critère de propagation.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor" id="méthodes-séparateurs"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#méthodes-séparateurs" title="Direct link to heading">#</a>Méthodes séparateurs</h4><p>Les S3VM (adaptation des SVM au semi-supervisé) prennent en compte les données non étiquetées pour choisir un meilleur séparateur passant par les régions les moins denses.</p><p>Ici les données non étiquetées sont prises en compte en mettant une &quot;fausse&quot; étiquette, mais cela rend le problème d&#x27;optimisation plus complexe.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor" id="méthodes-désacords"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#méthodes-désacords" title="Direct link to heading">#</a>Méthodes désacords</h4><p>Les méthodes basées sur des désacords utilisent la coopération entre plusieurs modèles.</p><p>La méthode la plus connue est le co-training. Elle consiste à entraîner plusieurs classifieurs (souvent deux), chacun basé sur une vue du corpus, puis à les améliorer entre eux à l’aide d’une masse importante de données non annotées. Les classifieurs plus à même de classer un exemple donné jouant le rôle de &quot;professeurs&quot; pour les autres.</p><p>Ces méthodes permettent naturellement d&#x27;allier apprentissage actif et apprentissage semi-supervisé.</p><p>En revanche, si les vues du corpus et donc les classifieurs sont dépendants entre eux, le co-training ne permettera pas d&#x27;améliorer les performances finales des modèles.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor" id="variante-transductive"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#variante-transductive" title="Direct link to heading">#</a>Variante transductive</h4><p>Il existe une variante transductive de l&#x27;apprentissage semi-supervisé où l&#x27;on ne cherche pas à généraliser à d&#x27;autres données mais à être le plus performant sur un jeu de données définit à l&#x27;avance.</p><p>Cela peut s&#x27;appliquer à l&#x27;imputation de données manquantes en concevant un modèle à partir des données présentes et à une mesure de similarité entre individus.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor" id="lapprentissage-inexact"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#lapprentissage-inexact" title="Direct link to heading">#</a>L&#x27;apprentissage inexact</h2><p>L&#x27;apprentissage inexact correspond au cas où la labelisation n&#x27;est pas faite au niveau de l&#x27;individu statistique mais à un autre niveau. Par exemple dans le cas du l&#x27;apprentissage multi-instance, c&#x27;est un groupe d&#x27;individu qui est étiqueté, on sait si un sac contient un individu négatif ou non.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor" id="lapprentissage-imprécis"></a><a aria-hidden="true" tabindex="-1" class="hash-link" href="#lapprentissage-imprécis" title="Direct link to heading">#</a>L&#x27;apprentissage imprécis</h2><p>L&#x27;apprentissage imprécis définit un environnement où l&#x27;information que l&#x27;on reçoit sur un individu puisse être faussée.</p><p>En effet pour certains problèmes d&#x27;apprentissage, on peut souhaiter une très grand nombre d&#x27;étiquettes en demandant celles-ci à une foule de personne. Cependant on peut s&#x27;exposer à des personnes qui se trompent par innatention ou manque de protocol d&#x27;étiquettages.</p></section></article><div class="margin-vert--xl"><nav class="pagination-nav"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/thesis/notes/2020/01/11/proofs"><h5 class="pagination-nav__link--sublabel">Previous Post</h5><h4 class="pagination-nav__link--label">« Some proofs about statistical learning and label noise</h4></a></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center">Copyright © 2020 Pierre Nodet.</div></div></footer>
</div>

<script src="/thesis/styles.bf205fd4.js"></script>

<script src="/thesis/runtime~main.3b69f82d.js"></script>

<script src="/thesis/main.da5a0b13.js"></script>

<script src="/thesis/common.a78ad88a.js"></script>

<script src="/thesis/2.5ec7a7d8.js"></script>

<script src="/thesis/ccc49370.b901c358.js"></script>

<script src="/thesis/0d256ad3.c052fd89.js"></script>


</body>
</html>