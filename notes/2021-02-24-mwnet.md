---
title: "Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting (MWNet)"
tags: [reading-notes,meta-learning,importance-reweighting]
---

<p align="center">

![MWNet](/figures/mwnet.png)

</p>

* **code** : https://github.com/xjtushujun/meta-weight-net
* **pdf** : https://papers.nips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf

1. MWNet is an actual biquality learning algorithm (uses both trusted and untrusted data).
2. Meta Learning algorithm to reweight untrusted samples by learning a reweighted loss function (weighting function with sample loss as an input and a reweight sample loss as the output).
3. The meta model (an MLP) estimate the reweighting function while the main model learned with the reweighted function.
4. Both models are jointly trained by solving bilevel optimization. The meta model is learned by how much the main model performs on trusted labels. The main model is learned by how much it performs on the loss reweighted by the meta model.
5. Theoritical proofs on convergence of the algorithm.
6. Extensive experiments against a lot of State-of-the-Art competitors

<!--truncate-->

## Assets

### Strengths

1. Small meta model (MLP) completly interpretable, as it's a simple reweighting function (plottable on the loss domain).
2. Sound paper and some details on how and why the method works.
3. No over the top algorithm complexity.
4. Works well on assymetric noise (flip noise) while it's a reweighting method.

### Drawbacks

1. No guarantees for the algorithm to work on Not At Random noise, as the meta model only uses loss values as an input to reweight it and no experiments have been conducted.
2. Meta Learning algorithm (can be tricky/hard to implement for a wide spread of neural network architectures and optimizers). Thanksfully some library solve this problem (see higher for PyTorch).

## Novelty

The approach is novel, it reuses the idea from L2RW (using meta learning to reweight samples), but instead of learning the weight of every samples, it smarty learns how to reweight the base loss.

<p align="center">

![MWNetLoss](/figures/mwnet-loss.png)

</p>

## Significance

Significant as a lot of results (detailed) show improvements against SotA, altough not that high against GLC a much simpler and sound algorithm. Theoritcal proofs are quite important for the field and have been reused by a lot of downstreams papers. 

## Soudness

Technically sound, proofs of main theorems proving convergence of the algorithm have been provided.

## Evaluation

Sufficient evaluation of the algorithm have been provided.

## How can I use/enhance the paper ?

1. I could enhance the paper by adapting it to Not At Random Noise.

## What did I learn from reading this paper ?

Deeper understanding of meta learning (how to do bilevel optimization schemes).

## New paper to read/Interesting Citations

1. Joint optimization framework for learning with noisy labels (CPVR 2018)

## How I am going to use this paper ?

To Implement

## Bibtex

```
@inproceedings{NEURIPS2019_e58cc5ca,
 author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting},
 url = {https://proceedings.neurips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf},
 volume = {32},
 year = {2019}
}
```