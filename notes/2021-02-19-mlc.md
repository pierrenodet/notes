---
title: "Meta Label Correction for Noisy Label Learning (MLC)"
tags: [reading-notes,meta-learning,label-correction]
---

<p align="center">

![MLC](/figures/mlc.png)

</p>

* **code** : https://github.com/microsoft/MLC
* **pdf** : https://www.microsoft.com/en-us/research/uploads/prod/2020/12/aaai2021_mlc_zheng.pdf

1. MLC is an actual biquality learning algorithm (uses both trusted and untrusted data).
2. MLC uses a Meta Learning approach to correct corrupted/untrusted labels.
3. The meta model (a label correction network) correct label for corrupted instances while the main model learns on corrected labels.
4. Both models are jointly trained by solving bilevel optimization. The meta model is learned by how much the main model performs on trusted labels. The main model is learned by how much it performs on labels corrected by the meta model.
5. A (novel) k step look ahead SGD to solve this optimization problem.
6. Meta Model has an embedding of the data and the untrusted label as an input to learn how to correct it.
7. Experiments on widely used datasets and noises against State-of-the-Art competitors.

<!--truncate-->

## Assets

### Strengths

1. MLC uses untrusted samples by correcting their labels, works well on assymetrical noise
2. Meta Model has an embedding of the data and the untrusted label as an input to learn how to correct it.
3. MLC and k-step look ahead SGD works well empirically.

### Drawbacks

1. Only agggregated results in experiments, hard to know when it's better than the competitors.
2. Costly algorithm especially with at least 5 steps ahead SGD, experiments are made with pretrained embeddigns (ResNet/BERT).
3. Lack of theoritical guarantees (bilevel optimization heuristic only works empirically).
4. Less interpretable than competitors, for example GLC has an explicit transition matrix to correct labels, MLC use a MLP (blackbox) over embeddings of a resnet/bert.
5. Correct Labels in an hard manner (don't use classifier confidence to do both label correction and instance reweighting).
6. Meta Learning algorithm (can be hard to implement for a wide spread of neural network architectures and optimizers). Thanksfully some library solve this problem (see higher for PyTorch).

## Novelty

It’s a Novel/Somewhat Novel approach that reapplies an idea found in another paper (mwnet) to learn how to relabel a noisy sample by using bilevel joint optimization. K step look ahead SGD might be novel too.

## Significance

The results are moderately significant as the improvements in accuracy are not huge over GLC which is a way simpler algorithm to both implement, test and interpret (not a blackbox MLP to correct labels). Moreover the results in experiments are quite aggregated so it’s not obvious where this method adds benefits (kind of label noise, strength of label noise, datasets).

## Soudness

The soundness of the article seems ok as it reuses a lot of results found in other papers to justify the algorithm.

## Evaluation

It's sufficient as it reuses a lot of ideas found in other papers and the reported performances seems ok.

## How can I use/enhance the paper ?

1.	Combine both instance reweighting and label correction, by either combining MWNet and MLC or use the soft labels instead of hard labels provided by the meta model (the weight comes from the meta model confidence).
2.  Doest it work well for not at random noise (it should but no experimentns have been conducted)

## What did I learn from reading this paper ?

Better understanding of how meta learning works.

## New paper to read/Interesting Citations

1. MWNet (predecessor)
2. DART (k-step look ahead SGD)

## How I am going to use this paper ?

To implement.

## Bibtex

```
@article{zheng2021mlc,
  title={Meta Label Correction for Noisy Label Learning},
  author={Zheng, Guoqing and Awadallah, Ahmed Hassan and Dumais, Susan},  
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  year={2021},
}
```