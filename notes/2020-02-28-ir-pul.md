---
title: "Importance Reweighting for Positive Unlabeled Learning"
---

After redoing the proofs of "Classification with Noisy Labels by Importance Reweighting" by Liu and Tao, we thought that extending their results to Positive Unlabeled Learning could be quite trivial and useful.

We are going to first remind everybody their results about importance reweighting for classification with asymetric constant label noise. Then we are going to show how Robust Label Learning (RLL) with asymetric noise relate to Positive Unlabeled Learning (PUL). Thus we will be able to show that we can adapt their results to learn on PUL data with constant propensity thanks to importance reweighting on a usual supervised surrogate loss.

<!--truncate-->

## Classification with Noisy Labels by Importance Reweighting

Let :
* $\mathcal{F} = {\mathcal{X}}^{\mathcal{Y}}$
* $\mathcal{L}={\mathcal{Y^2}}^{ℝ}$

If :

* Our labels are under a class dependent noise, with $(ρ_y)_{y∈\mathcal{Y}}∈[0,1]^{|\mathcal{Y}|}$

Then :

$$
∀L∈\mathcal{L},∃\tilde{L}∈\mathcal{L},\underset{f∈\mathcal{F}}{argmin}R_{D,L}(f)=\underset{f∈\mathcal{F}}{argmin}R_{D_ρ,\tilde{L}}(f)
$$

With :

$$
\tilde{L}(f(X),Y) = β(X,\tilde{Y})L(f(X),Y)
$$
$$
β(X,\tilde{Y}=y)=\frac{ℙ(\tilde{Y}=y|X)-ρ_{-y}}{(1-ρ_{+1}-ρ_{-1})ℙ(\tilde{Y}=y|X)}
$$

## Positive Unlabeled Learning is Robust Label Learning with asymetric noise

Some definition used in PUL :

- Let $\tilde{Y} ∈ \mathcal{\tilde{Y}} = \{?,+1\}$, be the observed label
- Let $π=ℙ(Y=1)$, be the class prior on the true unobserved label
- Let $\tilde{π}=ℙ(\tilde{Y}=1)$, be the class prior on the observed label
- Let $e(x)=ℙ(\tilde{Y}=1|Y=1,X=x)$, be the propensity function

Let's write the probability to see $\tilde{Y}=1$ and $\tilde{Y}=?$ in terms of the probability $Y=1$ and $Y=-1$ by using the definition of the propensity $e(x)$.

$$
ℙ(\tilde{Y}=1|X=x) = e(x)ℙ(Y=1|X=x)
$$

$$
ℙ(\tilde{Y}=?) = (1-e(x))ℙ(Y=1|X=x) + ℙ(Y=-1|X=x)
$$

Let's annonate all $?$ as $-1$ and express the probability to see $\tilde{Y}=y$ conditionally to $Y=-y$

$$
\begin{aligned}
ℙ(\tilde{Y}=1|Y=-1,X=x) &= e(x)ℙ(Y=1|Y=-1,X=x)
&= 0
\end{aligned}
$$

$$
\begin{aligned}
ℙ(\tilde{Y}=-1|Y=1,X=x) &= (1-e(x))ℙ(Y=1|Y=1,X=x) + ℙ(Y=-1|Y=1,X=x)
&= 1-e(x)
\end{aligned}
$$

With this rewriting we showed that PUL is equivalent to RLL with an asymetric noise function which is null for one of the label and equal to 1 minus the propensity for the other label.

## Importance Reweigthing for Postive Unlabeled Learning

The first result about Importance Reweighting for RLL only applies to a class dependent noise, so constant by instance. If we want to apply this result to PUL thanks to second point, it will only be valid for PUL with a constant propensity function.

So $∃e∈ℝ,∀x∈\mathcal{X},e(x)=e$ and $ρ_1=0$ and $ρ_{-1}=1-e$.

Let's calculate $β$ for PUL :

$$
\begin{aligned}
β(X,\tilde{Y}=1) &= \frac{ℙ(\tilde{Y}=1|X)-ρ_{-1}}{(1-ρ_{+1}-ρ_{-1})ℙ(\tilde{Y}=1|X)}\\
&= \frac{ℙ(\tilde{Y}=1|X)-(1-e)}{(1-(1-e))ℙ(\tilde{Y}=1|X)}\\
&= \frac{ℙ(\tilde{Y}=1|X)-(1-e)}{eℙ(\tilde{Y}=1|X)}\\
&= \frac{1}{e}-\frac{1-e}{eℙ(\tilde{Y}=1|X)}\\
&= \frac{1}{e}\left(1-\frac{1-e}{ℙ(\tilde{Y}=1|X)}\right)\\
\end{aligned}
$$

$$
\begin{aligned}
β(X,\tilde{Y}=-1) &= \frac{ℙ(\tilde{Y}=-1|X)-ρ_1}{(1-ρ_{+1}-ρ_{-1})ℙ(\tilde{Y}=-1|X)}\\
&= \frac{ℙ(\tilde{Y}=-1|X)}{(1-(1-e))ℙ(\tilde{Y}=-1|X)}\\
&= \frac{1}{e}\\
\end{aligned}
$$

In the PUL setting, we are able to express $β$ in function of the propensity $e$ instead of the noise probability functions $ρ_1$ and $ρ_{-1}$.

Now, if we want to adapt a loss function from the supervised setting to PUL by importance reweighting, we only need to estimate the propensity constant $e$.

## Propensity Estimation

The next step is to benchmark all propensity estimation method, compare it to flip probabilities estimation and propose a new estimation without hypothesis on the data so we have an end to end PUL learning framework without hypothesis on the data.