<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="generator" content="Docusaurus">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css">

<title data-react-helmet="true">Blog</title>

<meta data-react-helmet="true" http-equiv="x-ua-compatible" content="ie=edge"><meta data-react-helmet="true" property="og:title" content="Blog"><meta data-react-helmet="true" name="description" content="Blog"><meta data-react-helmet="true" property="og:description" content="Blog"><meta data-react-helmet="true" name="twitter:card" content="summary">

<link data-react-helmet="true" rel="shortcut icon" href="/thesis/img/favicon.ico">


<link rel="stylesheet" href="/thesis/styles.809cbcd8.css">

</head>
<body>

<div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a aria-current="page" class="navbar__brand active" href="/thesis/"><img class="navbar__logo" src="/thesis/img/logo.png" alt="thesis logo"><strong>thesis</strong></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/thesis/manuscript/introduction">Manuscript</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/thesis/notes">Reading Notes</a><a class="navbar__item navbar__link" target="_blank" rel="noopener noreferrer" href="https://github.com/pierrenodet/thesis">GitHub</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a aria-current="page" class="navbar__brand active" href="/thesis/"><img class="navbar__logo" src="/thesis/img/logo.png" alt="thesis logo"><strong>thesis</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/thesis/manuscript/introduction">Manuscript</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/thesis/notes">Reading Notes</a></li><li class="menu__list-item"><a class="menu__link" target="_blank" rel="noopener noreferrer" href="https://github.com/pierrenodet/thesis">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="container margin-vert--xl"><div class="row"><div class="col col--8 col--offset-2"><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_2RZH"><a href="/thesis/notes/2020/02/29/mutual-information">Flexible Biquality Learning with Mutual Information</a></h2><div class="margin-bottom--sm"><time datetime="2020-02-29T00:00:00.000Z" class="blogPostDate_3tRe">February 29, 2020</time></div><div class="avatar margin-bottom--md"><div class="avatar__intro"></div></div></header><section class="markdown"><p>The concept of Biquality Data has been introduced in &quot;Unifying Semi-Supervised and Robust Learning by Mixup&quot; by Hataya and Nakayama where there is two sets of data, a trusted and an untrusted one.</p><p>The trusted dataset (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">D_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>) contains labels <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">Y_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> that represents the true concept (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>T</mi></msub><mi mathvariant="normal">∣</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">Y_T|X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07847em">X</span></span></span></span></span>) to learn on the learning task. The untrusted dataset (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>U</mi></msub></mrow><annotation encoding="application/x-tex">D_U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em">U</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>) contains labels <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>U</mi></msub></mrow><annotation encoding="application/x-tex">Y_U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em">U</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> that range from being uniformative (as in Unsupervised Learning) to being the same as the trusted dataset (as in Supervised Learning).</p><p>In order to represents where the untrusted labels lie in this range, we define the quality of the untrusted labels. In the previous paper, they used a ratio of Kullback-Leibler divergence. Without critising the chosen formula, they were unable in the paper to directly use this value in the learning process.</p></section><footer class="row margin-vert--lg"><div class="col text--right"><a aria-label="Read more about Flexible Biquality Learning with Mutual Information" href="/thesis/notes/2020/02/29/mutual-information"><strong>Read More</strong></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_2RZH"><a href="/thesis/notes/2020/02/28/ir-pul">Importance Reweighting for Positive Unlabeled Learning</a></h2><div class="margin-bottom--sm"><time datetime="2020-02-28T00:00:00.000Z" class="blogPostDate_3tRe">February 28, 2020</time></div><div class="avatar margin-bottom--md"><div class="avatar__intro"></div></div></header><section class="markdown"><p>After redoing the proofs of &quot;Classification with Noisy Labels by Importance Reweighting&quot; by Liu and Tao, we thought that extending their results to Positive Unlabeled Learning could be quite trivial and useful.</p><p>We are going to first remind everybody their results about importance reweighting for classification with asymetric constant label noise. Then we are going to show how Robust Label Learning (RLL) with asymetric noise relate to Positive Unlabeled Learning (PUL). Thus we will be able to show that we can adapt their results to learn on PUL data with constant propensity thanks to importance reweighting on a usual supervised surrogate loss.</p></section><footer class="row margin-vert--lg"><div class="col text--right"><a aria-label="Read more about Importance Reweighting for Positive Unlabeled Learning" href="/thesis/notes/2020/02/28/ir-pul"><strong>Read More</strong></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_2RZH"><a href="/thesis/notes/2020/01/18/bekker">Bekker - Learning from Positive and Unlabeled Data - A Survey</a></h2><div class="margin-bottom--sm"><time datetime="2020-01-18T00:00:00.000Z" class="blogPostDate_3tRe">January 18, 2020</time></div><div class="avatar margin-bottom--md"><div class="avatar__intro"></div></div></header><section class="markdown"></section><footer class="row margin-vert--lg"><div class="col text--right"><a aria-label="Read more about Bekker - Learning from Positive and Unlabeled Data - A Survey" href="/thesis/notes/2020/01/18/bekker"><strong>Read More</strong></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_2RZH"><a href="/thesis/notes/2020/01/11/proofs">Some proofs about Statistical Learning and Label Noise</a></h2><div class="margin-bottom--sm"><time datetime="2020-01-11T00:00:00.000Z" class="blogPostDate_3tRe">January 11, 2020</time></div><div class="avatar margin-bottom--md"><div class="avatar__intro"></div></div></header><section class="markdown"><p>When doing binary classfication, having noise in our label means that they can be flipped from one class to the other. When observing a label, we will never know if it has been flipped or not, only the probability of the flip to occur.</p><p>Noisy labels can definitly mess up with model accuracy and make us unable to evaluate it. For example, answering this question can be quite hard : Is it a false positive or the algorithm was right and the label was flipped ?</p><p>Thanksfully researchers are clever and they found some theoritical guarantees to still be able to do machine learning in this setup. We will redo proofs found in various papers covering this subject, starting from the easiest setup (random noise) to the hardest one (individual dependent noise).</p></section><footer class="row margin-vert--lg"><div class="col text--right"><a aria-label="Read more about Some proofs about Statistical Learning and Label Noise" href="/thesis/notes/2020/01/11/proofs"><strong>Read More</strong></a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_2RZH"><a href="/thesis/notes/2019/09/27/zhou">Zhou - A Brief Introduction to Weakly Supervised Learning</a></h2><div class="margin-bottom--sm"><time datetime="2019-09-27T00:00:00.000Z" class="blogPostDate_3tRe">September 27, 2019</time></div><div class="avatar margin-bottom--md"><div class="avatar__intro"></div></div></header><section class="markdown"><p>Cet article est une survey sur l&#x27;apprentissage faiblement supervisé.</p><p>Trois types d&#x27;apprentissage faiblement supervisés sont décrits :</p><ol><li>L&#x27;apprentissage supervisé incomplet</li><li>L&#x27;apprentissage supervisé inexact</li><li>L&#x27;apprentissage supervisé imprécis</li></ol></section><footer class="row margin-vert--lg"><div class="col text--right"><a aria-label="Read more about Zhou - A Brief Introduction to Weakly Supervised Learning" href="/thesis/notes/2019/09/27/zhou"><strong>Read More</strong></a></div></footer></article><nav class="pagination-nav"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center">Copyright © 2020 Pierre Nodet.</div></div></footer>
</div>

<script src="/thesis/styles.63942fde.js"></script>

<script src="/thesis/runtime~main.a1a255aa.js"></script>

<script src="/thesis/main.f5bff3c7.js"></script>

<script src="/thesis/common.5db92d7a.js"></script>

<script src="/thesis/2.399233f5.js"></script>

<script src="/thesis/a6aa9e1f.87d4a767.js"></script>

<script src="/thesis/7232466b.754d3f58.js"></script>

<script src="/thesis/0b2e5ec8.724a7fde.js"></script>

<script src="/thesis/7283ae69.1047b9cf.js"></script>

<script src="/thesis/8256a383.db033057.js"></script>

<script src="/thesis/5a99edf6.fd2bbdb5.js"></script>

<script src="/thesis/22bcd24a.56139a2a.js"></script>


</body>
</html>